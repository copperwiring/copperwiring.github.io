[{"authors":["admin"],"categories":null,"content":"I am currently a graduate research assistant at Networked Robotics and Sensing Laboratory under the supervision of Prof. Shahram Payandeh at Simon Fraser University, Canada. My work revolves around intersection of computer vision and machine learning where I address the problem of unsupervised tracking using RGB and depth images using depth image sensors. In past, I have part of Helicopter and VTOL Laboratory, IIT Kanpur (supervised by Prof. Abhishek), Samsung IoT Innovation Lab, IIT Delhi (supervised by Prof. Brejesh Lall) and Applied Cognitive Science Lab, IIT Mandi (supervised by Prof. Varun Dutt).\nI am also currently doing an internship at UrtheCast where I am responsible for developing computationally intensive image processing algorithms for large scale satellite data within a cloud-based infrastructure using deep learning.\nOver the course of my work experience, have developed excellent skills in PyTorch, TensorFlow, Python, MATLAB, Numpy, Scipy, OpenCV, Matplotlib, GDAL, etc. scientific stack as well as cloud services like AWS.\nI am active member of developer community groups of Vancouver where I run Women in Machine Learning and Data Science meetup, Google Developers Group and Vancouver OpenCV meetup. As a strong proponent of tech and diversity, my involvement goes beyond local community work. I am currently one of the the chairs of Women in Computer Vision workshop co-hosted with CVPR, 2020 and was on the committe of the Women in Machine Learning workshop, 2019.\nIf you have questions about my any of work, feel free to reach out to me via email. I\u0026rsquo;ll be happy to chat!\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://srishti.dev/author/srishti-yadav/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/srishti-yadav/","section":"authors","summary":"I am currently a graduate research assistant at Networked Robotics and Sensing Laboratory under the supervision of Prof. Shahram Payandeh at Simon Fraser University, Canada. My work revolves around intersection of computer vision and machine learning where I address the problem of unsupervised tracking using RGB and depth images using depth image sensors.","tags":null,"title":"Srishti Yadav","type":"authors"},{"authors":null,"categories":null,"content":"In deep learning, it is not easy to tune hyperparameters for optimal results. If we have 2 parameters (each with 3 prior desirable values), it is an easier problem. We will have possible combinations to try. However, with more parameters, the possible combinations will increase exponentially. For example, for 5 parameters, each with 4 desired values, we will have possible combinations. Manually trying each of them is not a very practical approach.\nhence, the question usually is \u0026ldquo;Which way should I update my hyper-parameter to reduce the loss (i.e. gradients) in order to find the optimal model architecture?\nIdealy, we should come up with an approach :\n where we can search for hyperparameters (hyper-parameter search space) using a distributed compute on the cloud. intelligently optimize which of the possible combinations from the search space will give us the best results. Supports exisitng optimization techniques like Grid Search, Random Search, Bayesian Optmization etc.  Some of the exisiting tools for hyperparamter tuning are:\n RayTune (https://ray.readthedocs.io/en/latest/tune.html) Talos (https://autonomio.github.io/talos) NNI (https://nni.readthedocs.io/en/latest/index.html) Orion (https://orion.readthedocs.io/en/latest/user/pytorch.html#adapting-the-code-for-orion) Sherpa (https://parameter-sherpa.readthedocs.io/en/latest/parallel/parallel-guide.html) TPOT (https://github.com/EpistasisLab/tpot) Hyperopt (https://github.com/hyperopt/hyperopt) AWS Sagemaker Hyperparameter tuning (https://aws.amazon.com/sagemaker/?hp=tile\u0026amp;so-exp=below) Botorch (https://botorch.org/)  Here, we will discuss hyperopt!\nHyperopt is an open-source hyperparameter tuning library written for Python. Hyperopt provides a general API for searching over hyperparameters and model types. Hyperopt offers two tuning algorithms: Random Search and the Bayesian method Tree of Parzen Estimators (TPE). To run hyperopt you define:\n the objective function the parameter space the number of experiments  There are both continuous and categorical methods to describe the parameters.\nLimitation: Hyper-parameter takes a lot of time to tune the parameters if number of trials and number of epocs (iteration of the neural network) are higher (which is desired). Hence, it would be good to explore how to parallelize the tuning work.\n","date":1591040280,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591040280,"objectID":"f1a43b65d43100ada01b052d9aa96095","permalink":"https://srishti.dev/post/2020-06-01-hyperopt/","publishdate":"2020-06-01T12:38:00-07:00","relpermalink":"/post/2020-06-01-hyperopt/","section":"post","summary":"In deep learning, it is not easy to tune hyperparameters for optimal results. If we have 2 parameters (each with 3 prior desirable values), it is an easier problem. We will have possible combinations to try.","tags":"technical","title":"Hyperopt: A tool for parameter tuning","type":"post"},{"authors":null,"categories":null,"content":"When we do machine learning, a lot of time, we use vectors to perform any computation. let us see few examples of vectors we come across any machine learning or dep leaning mathematics:\nExample 1: $$x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}.$$\nExample 2: (ax^2 + bx + c = 0)\nThe same thing can be done using the traditional method of loops too. Let\u0026rsquo;s see how.\nWe will write a small code to take the dot problem of two numbers. For the vectorized method, we\u0026rsquo;ll use numpy library and for non-vectorized method, we\u0026rsquo;ll use the traditional for loop.\nVectorized Method: import numpy as np import time #We'll create an array of the size 100000 and populate it with random samples from a uniform distribution over [0, 1). #For this purpose, we'l use np.random.rand() #Link: https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.random.rand.html a = np.random.rand(100000) b = np.random.rand(100000) #We'll compute the time taken to compute the vector dot product tic = time.time() c = np.dot(a,b) toc = time.time() print(c) print(\u0026quot;Vectorized version:\u0026quot; + str(1000*(toc-tic)) + \u0026quot; ms\u0026quot;)  Output:\n25032.80826579146 Vectorized version:5.956172943115234 ms  Non-Vectorized Method: ## Non-Vectorized Version c = 0; tic = 0; toc = 0 #We'll compute the time taken to compute the dot product using for loop tic = time.time() for i in range(100000): c += a[i]*b[i] toc = time.time() print(c) print(\u0026quot;Vectorized version:\u0026quot; + str(1000*(toc-tic)) + \u0026quot; ms\u0026quot;)  Output:\n25032.808265791115 Vectorized version:91.36772155761719 ms  From the outputs, we can see that the for loop takes 20x time than the vectorized method. This difference can be significant if we have large amounts of data where vectorization can save us a lot of computation time. Hence, the need of vectorization!\n","date":1573414680,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573414680,"objectID":"53f1c23bad5cbaa419db65b80887fe1e","permalink":"https://srishti.dev/post/2019-11-10-why-do-we-use-vectorization-in-ml/","publishdate":"2019-11-10T12:38:00-07:00","relpermalink":"/post/2019-11-10-why-do-we-use-vectorization-in-ml/","section":"post","summary":"When we do machine learning, a lot of time, we use vectors to perform any computation. let us see few examples of vectors we come across any machine learning or dep leaning mathematics:","tags":"technical","title":"Vectorization in machine learning","type":"post"},{"authors":null,"categories":null,"content":"In computer vision we keep referring to feature vector, especially when we talk about CNNs. But what exactly is a feature vector and can how can you visualize it? Imagine we have an RGB image is the form as shown below:\nNotice, that the value are unique for each color channel. However, we don’t intend to send out this data to any algorithm in 3 fold i.e. sending for one channel and doing computation, sending for second and so on. This is very unrealistic from computation point of you because our original data (pixel values) are ‘together’ representing the image. Hence, we create what we call a feature vector. We create 1-D column vector with values from these 3 color channels. For instance, we start from Red channel and bread each value row wise till the last value. Next we do the same for Blue channel and lastly for Green channel. This gives us one single column vector representing the original image in the form of, what we call, ‘feature vector’. So, what is the final size of the feature vector? This is important to know the total amount of data we will eventually use to do any computation.\nWhen we see the CNN structure, we see that the architecture is defined usingm x n x 3, in case of RGB images. For example, a CNN architecture of dimension 64 x 64 x 3 will have a feature vector of size 12,288.\nSimple!\n","date":1573190466,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573190466,"objectID":"5e4e134262ccb12133b776e4267ba6c2","permalink":"https://srishti.dev/post/2019-11-07-feature-vector-in-machine-learning/","publishdate":"2019-11-07T22:21:06-07:00","relpermalink":"/post/2019-11-07-feature-vector-in-machine-learning/","section":"post","summary":"In computer vision we keep referring to feature vector, especially when we talk about CNNs. But what exactly is a feature vector and can how can you visualize it? Imagine we have an RGB image is the form as shown below:","tags":"technical","title":"Feature Vector in Machine Learning","type":"post"},{"authors":null,"categories":null,"content":"If you are using Ubuntu subshell in Windows (I use the one available in Microsoft Store), this is for you. There are times, where you may need to access the files saved on your windows in the Linux subshell. All you need to do is mount your drive. You may need to access the subshell via administrative privileges. For example, if you have the file in C: Drive, use the following:\ncd /mnt/c ls  The same can be used to access data from external hard disk too. If the external disk is in D: Drive, the command would simply change to:\ncd /mnt/d ls  ","date":1573028706,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573028706,"objectID":"0e7daaa6bc9c277865aaa45019391998","permalink":"https://srishti.dev/post/2019-11-06-accessing-windows-from-linux/","publishdate":"2019-11-06T01:25:06-07:00","relpermalink":"/post/2019-11-06-accessing-windows-from-linux/","section":"post","summary":"If you are using Ubuntu subshell in Windows (I use the one available in Microsoft Store), this is for you. There are times, where you may need to access the files saved on your windows in the Linux subshell.","tags":"Geekout","title":"Accessing Windows File From Linux Subshell","type":"post"},{"authors":["Maryamsadat Rasoulidanesh","Srishti Yadav","Sachini Herath","Yasaman Vaghei","Shahram Payandeh"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"69f20fce694a3d92aebf009185695fcd","permalink":"https://srishti.dev/publication/deep-attention-models-for-human-tracking-using-rgbd/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/deep-attention-models-for-human-tracking-using-rgbd/","section":"publication","summary":"Visual tracking performance has long been limited by the lack of better appearance models. These models fail either where they tend to change rapidly, like in motion-based tracking, or where accurate information of the object may not be available, like in color camouflage (where background and foreground colors are similar). This paper proposes a robust, adaptive appearance model which works accurately in situations of color camouflage, even in the presence of complex natural objects. The proposed model includes depth as an additional feature in a hierarchical modular neural framework for online object tracking. The model adapts to the confusing appearance by identifying the stable property of depth between the target and the surrounding object(s). The depth complements the existing RGB features in scenarios when RGB features fail to adapt, hence becoming unstable over a long duration of time. The parameters of the model are learned efficiently in the Deep network, which consists of three modules:(1) The spatial attention layer, which discards the majority of the background by selecting a region containing the object of interest; (2) the appearance attention layer, which extracts appearance and spatial information about the tracked object; and (3) the state estimation layer, which enables the framework to predict future object appearance and location. Three different models were trained and tested to analyze the effect of depth along with RGB information. Also, a model is proposed to utilize only depth as a standalone input for tracking purposes. The proposed models were also evaluated in real-time using KinectV2 and showed very promising results. The results of our proposed network structures and their comparison with the state-of-the-art RGB tracking model demonstrate that adding depth significantly improves the accuracy of tracking in a more challenging environment (i.e., cluttered and camouflaged environments). Furthermore, the results of depth-based models showed that depth data can provide enough information for accurate tracking, even without RGB information.","tags":["Source Themes"],"title":" Deep Attention Models for Human Tracking Using RGBD ","type":"publication"},{"authors":["Srishti Yadav","Shahram Payandeh"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1541030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541030400,"objectID":"dc8da6b4fe376e635f00805ad77a29f7","permalink":"https://srishti.dev/publication/kcfexperimentalstudy/","publishdate":"2018-11-01T00:00:00Z","relpermalink":"/publication/kcfexperimentalstudy/","section":"publication","summary":"Over years correlation filter-based trackers have proved their worth with their increased efficiency and increased computation speed. Kernelized Correlation Filter (KCF) was one such attempt which, by using kernel trick, achieved compelling result as compared to traditional correlation filter-based trackers. In this paper, our goal is to analyze this tracker to observe its strengths and weaknesses in detail. We use Kinect RGB camera for our experimental analysis and report our findings. The analysis showed that KCF is not only computationally very fast, it is time-invariant and very robust to speed and vertical motions. However, it is not very robust to illumination variations, scale and color.","tags":["Source Themes"],"title":"Real-Time Experimental Study of Kernelized Correlation Filter Tracker using RGB Kinect Camera","type":"publication"},{"authors":["Pratik Chaturvedi","Kamal Kishore Thakur","Naresh Mali","Venkata Uday Kala","Sudhakar Kumar","Srishti Yadav","Varun Dutt"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525132800,"objectID":"68c2049ad763407b982f7539fa0a174e","permalink":"https://srishti.dev/publication/a-low-cost-iot-framework-for-landslide-prediction-and-risk-communication/","publishdate":"2018-05-01T00:00:00Z","relpermalink":"/publication/a-low-cost-iot-framework-for-landslide-prediction-and-risk-communication/","section":"publication","summary":"In this book chapter, we propose the design and development of a low-cost IoT framework formonitoring landslide is discussed. This framework involved the use of MEMS-based sensors for monitoring landslides at the lab scale. The proposed frame-work can monitor soil moisture and movement and generate alerts based onpredefined thresholds.","tags":["Source Themes"],"title":"A Low-Cost IoT Framework for Landslide Prediction and Risk Communication","type":"publication"},{"authors":["Mali Naresh","Pratik Chaturvedi","Srishti Yadav","Varun Dutt","Kala Venkat Uday"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498867200,"objectID":"2b0df95e8b670a5265685b036c94aacc","permalink":"https://srishti.dev/publication/training-of-sensors-for-early-warning-system-of-rainfall-induced-landslides/","publishdate":"2017-12-01T00:00:00Z","relpermalink":"/publication/training-of-sensors-for-early-warning-system-of-rainfall-induced-landslides/","section":"publication","summary":"Changes in the Earth’s climate are likely to increase natural hazards such as drought, floods, earthquakes,landslides, etc. The present study focusing on to early warning systems (EWS) of landslides, major issues in Himalayan regionwithout prominence to deforestation, encroachments and un-engineered cutting of slopes and reforming for infrastructuralpurposes. EWS can be depicted by conducting a series of flume tests using micro-electro mechanical systems sensors data afterreaching threshold values under controlled laboratory conditions. Based on the threshold value database, an alert will be sentvia SMS","tags":["Source Themes"],"title":"Training of Sensors for Early Warning System of Rainfall Induced Landslides","type":"publication"}]