<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Srishti Yadav</title>
    <link>https://srishti.dev/</link>
      <atom:link href="https://srishti.dev/index.xml" rel="self" type="application/rss+xml" />
    <description>Srishti Yadav</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 01 Jun 2020 12:38:00 -0700</lastBuildDate>
    <image>
      <url>https://srishti.dev/images/icon_hu83f24c6794b0c645cb0fb9e8b29ab31d_111941_512x512_fill_lanczos_center_2.png</url>
      <title>Srishti Yadav</title>
      <link>https://srishti.dev/</link>
    </image>
    
    <item>
      <title>Hyperopt: A tool for parameter tuning</title>
      <link>https://srishti.dev/post/2020-06-01-hyperopt/</link>
      <pubDate>Mon, 01 Jun 2020 12:38:00 -0700</pubDate>
      <guid>https://srishti.dev/post/2020-06-01-hyperopt/</guid>
      <description>&lt;p&gt;In deep learning, it is not easy to tune hyperparameters for optimal results. If we have 2 parameters (each with 3 prior desirable values), it is an easier problem. We will have possible combinations to try. However, with more parameters, the possible combinations will increase exponentially. For example, for 5 parameters, each with 4 desired values, we will have possible combinations. Manually trying each of them is not a very practical approach.&lt;/p&gt;
&lt;p&gt;hence, the question usually is &amp;ldquo;Which way should I update my hyper-parameter to reduce the loss (i.e. gradients) in order to find the optimal model architecture?&lt;/p&gt;
&lt;p&gt;Idealy, we should come up with an approach :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;where we can search for hyperparameters (hyper-parameter search space) using a distributed compute on the cloud.&lt;/li&gt;
&lt;li&gt;intelligently optimize which of the possible combinations from the search space will give us the best results.&lt;/li&gt;
&lt;li&gt;Supports exisitng optimization techniques like Grid Search, Random Search, Bayesian Optmization etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some of the exisiting tools for hyperparamter tuning are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;RayTune (&lt;a href=&#34;https://ray.readthedocs.io/en/latest/tune.html&#34;&gt;https://ray.readthedocs.io/en/latest/tune.html&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Talos (&lt;a href=&#34;https://autonomio.github.io/talos&#34;&gt;https://autonomio.github.io/talos&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;NNI (&lt;a href=&#34;https://nni.readthedocs.io/en/latest/index.html&#34;&gt;https://nni.readthedocs.io/en/latest/index.html&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Orion (&lt;a href=&#34;https://orion.readthedocs.io/en/latest/user/pytorch.html#adapting-the-code-for-orion&#34;&gt;https://orion.readthedocs.io/en/latest/user/pytorch.html#adapting-the-code-for-orion&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Sherpa (&lt;a href=&#34;https://parameter-sherpa.readthedocs.io/en/latest/parallel/parallel-guide.html&#34;&gt;https://parameter-sherpa.readthedocs.io/en/latest/parallel/parallel-guide.html&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;TPOT (&lt;a href=&#34;https://github.com/EpistasisLab/tpot&#34;&gt;https://github.com/EpistasisLab/tpot&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Hyperopt (&lt;a href=&#34;https://github.com/hyperopt/hyperopt&#34;&gt;https://github.com/hyperopt/hyperopt&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;AWS Sagemaker Hyperparameter tuning (&lt;a href=&#34;https://aws.amazon.com/sagemaker/?hp=tile&amp;amp;so-exp=below&#34;&gt;https://aws.amazon.com/sagemaker/?hp=tile&amp;amp;so-exp=below&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Botorch (&lt;a href=&#34;https://botorch.org/&#34;&gt;https://botorch.org/&lt;/a&gt;)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here, we will discuss hyperopt!&lt;/p&gt;
&lt;p&gt;Hyperopt is an open-source hyperparameter tuning library written for Python. Hyperopt provides a general API for searching over hyperparameters and model types. Hyperopt offers two tuning algorithms: Random Search and the Bayesian method Tree of Parzen Estimators (TPE). To run hyperopt you define:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the objective function&lt;/li&gt;
&lt;li&gt;the parameter space&lt;/li&gt;
&lt;li&gt;the number of experiments&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are both continuous and categorical methods to describe the parameters.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://srishti.dev/img/hyperopt.png &#34; width=&#34;600&#34; height=&#34;800&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Limitation&lt;/strong&gt;:
Hyper-parameter takes a lot of time to tune the parameters if number of trials and number of epocs (iteration of the neural network) are higher (which is desired). Hence, it would be good to explore how to parallelize the tuning work.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Vectorization in machine learning</title>
      <link>https://srishti.dev/post/2019-11-10-why-do-we-use-vectorization-in-ml/</link>
      <pubDate>Sun, 10 Nov 2019 12:38:00 -0700</pubDate>
      <guid>https://srishti.dev/post/2019-11-10-why-do-we-use-vectorization-in-ml/</guid>
      <description>&lt;p&gt;When we do machine learning, a lot of time, we use vectors to perform any computation. let us see few examples of vectors we come across any machine learning or dep leaning mathematics:&lt;/p&gt;
&lt;p&gt;Example 1: $$x = {-b \pm \sqrt{b^2-4ac} \over 2a}.$$&lt;/p&gt;
&lt;p&gt;Example 2: (ax^2 + bx + c = 0)&lt;/p&gt;
&lt;p&gt;The same thing can be done using the traditional method of loops too. Let&amp;rsquo;s see how.&lt;/p&gt;
&lt;p&gt;We will write a small code to take the dot problem of two numbers. For the vectorized method, we&amp;rsquo;ll use numpy library and for non-vectorized method, we&amp;rsquo;ll use the traditional for loop.&lt;/p&gt;
&lt;h1 id=&#34;vectorized-method&#34;&gt;Vectorized Method:&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;import numpy as np
import time

#We&#39;ll create an array of the size 100000 and populate it with random samples from a uniform distribution over [0, 1).
#For this purpose, we&#39;l use np.random.rand()
#Link: https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.random.rand.html
a =  np.random.rand(100000)
b =  np.random.rand(100000)

#We&#39;ll compute the time taken to compute the vector dot product
tic = time.time()
c = np.dot(a,b)
toc = time.time()

print(c)
print(&amp;quot;Vectorized version:&amp;quot; + str(1000*(toc-tic)) + &amp;quot; ms&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Output:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;25032.80826579146
Vectorized version:5.956172943115234 ms
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;non-vectorized-method&#34;&gt;Non-Vectorized Method:&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;## Non-Vectorized Version

c = 0; tic = 0; toc = 0

#We&#39;ll compute the time taken to compute the dot product using for loop
tic = time.time()
for i in range(100000):
    c += a[i]*b[i]
toc = time.time()

print(c)
print(&amp;quot;Vectorized version:&amp;quot; + str(1000*(toc-tic)) + &amp;quot; ms&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Output:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;25032.808265791115
Vectorized version:91.36772155761719 ms
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the outputs, we can see that the for loop takes 20x time than the vectorized method. This difference can be significant if we have large amounts of data where vectorization can save us a lot of computation time. Hence, the need of vectorization!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Feature Vector in Machine Learning</title>
      <link>https://srishti.dev/post/2019-11-07-feature-vector-in-machine-learning/</link>
      <pubDate>Thu, 07 Nov 2019 22:21:06 -0700</pubDate>
      <guid>https://srishti.dev/post/2019-11-07-feature-vector-in-machine-learning/</guid>
      <description>&lt;p&gt;In computer vision we keep referring to feature vector, especially when we talk about CNNs. But what exactly is a feature vector and can how can you visualize it?
Imagine we have an RGB image is the form as shown below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://srishti.dev/img/rgbchannel.png &#34; width=&#34;150&#34; height=&#34;120&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice, that the value are unique for each color channel. However, we don’t intend to send out this data to any algorithm in 3 fold i.e. sending for one channel and doing computation, sending for second and so on. This is very unrealistic from computation point of you because our original data (pixel values) are ‘together’ representing the image. Hence, we create what we call a feature vector. We create 1-D column vector with values from these 3 color channels. For instance, we start from Red channel and bread each value row wise till the last value. Next we do the same for Blue channel and lastly for Green channel. This gives us one single column vector representing the original image in the form of, what we call, ‘feature vector’. So, what is the final size of the feature vector? This is important to know the total amount of data we will eventually use to do any computation.&lt;/p&gt;
&lt;p&gt;When we see the CNN structure, we see that the architecture is defined usingm x n x 3, in case of RGB images. For example, a CNN architecture of dimension 64 x 64 x 3 will have a feature vector of size 12,288.&lt;/p&gt;
&lt;p&gt;Simple!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Accessing Windows File From Linux Subshell</title>
      <link>https://srishti.dev/post/2019-11-06-accessing-windows-from-linux/</link>
      <pubDate>Wed, 06 Nov 2019 01:25:06 -0700</pubDate>
      <guid>https://srishti.dev/post/2019-11-06-accessing-windows-from-linux/</guid>
      <description>&lt;p&gt;If you are using Ubuntu subshell in Windows (I use the one available in Microsoft Store), this is for you. There are times, where you may need to access the files saved on your windows in the Linux subshell.
All you need to do is mount your drive. You may need to access the subshell via administrative privileges. For example, if you have the file in C: Drive, use the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /mnt/c
ls
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The same can be used to access data from external hard disk too. If the external disk is in D: Drive, the command would simply change to:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /mnt/d
ls
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title> Deep Attention Models for Human Tracking Using RGBD (Poster @NeurIPS, 2020)</title>
      <link>https://srishti.dev/publication/deep-attention-models-for-human-tracking-using-rgbd/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://srishti.dev/publication/deep-attention-models-for-human-tracking-using-rgbd/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Real-Time Experimental Study of Kernelized Correlation Filter Tracker using RGB Kinect Camera</title>
      <link>https://srishti.dev/publication/kcfexperimentalstudy/</link>
      <pubDate>Thu, 01 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://srishti.dev/publication/kcfexperimentalstudy/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>A Low-Cost IoT Framework for Landslide Prediction and Risk Communication</title>
      <link>https://srishti.dev/publication/a-low-cost-iot-framework-for-landslide-prediction-and-risk-communication/</link>
      <pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate>
      <guid>https://srishti.dev/publication/a-low-cost-iot-framework-for-landslide-prediction-and-risk-communication/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Training of Sensors for Early Warning System of Rainfall Induced Landslides</title>
      <link>https://srishti.dev/publication/training-of-sensors-for-early-warning-system-of-rainfall-induced-landslides/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://srishti.dev/publication/training-of-sensors-for-early-warning-system-of-rainfall-induced-landslides/</guid>
      <description>&lt;!-- 
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). --&gt;
</description>
    </item>
    
  </channel>
</rss>
