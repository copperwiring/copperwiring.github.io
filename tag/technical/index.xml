<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>technical | Srishti Yadav</title>
    <link>https://srishti.dev/tag/technical/</link>
      <atom:link href="https://srishti.dev/tag/technical/index.xml" rel="self" type="application/rss+xml" />
    <description>technical</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 10 Nov 2019 12:38:00 -0700</lastBuildDate>
    <image>
      <url>https://srishti.dev/images/icon_hu83f24c6794b0c645cb0fb9e8b29ab31d_111941_512x512_fill_lanczos_center_2.png</url>
      <title>technical</title>
      <link>https://srishti.dev/tag/technical/</link>
    </image>
    
    <item>
      <title>Why do we use vectorization in Machine Learning?</title>
      <link>https://srishti.dev/post/2019-11-10-why-do-we-use-vectorization-in-ml/</link>
      <pubDate>Sun, 10 Nov 2019 12:38:00 -0700</pubDate>
      <guid>https://srishti.dev/post/2019-11-10-why-do-we-use-vectorization-in-ml/</guid>
      <description>&lt;p&gt;Example 1: $$x = {-b \pm \sqrt{b^2-4ac} \over 2a}.$$&lt;/p&gt;
&lt;p&gt;Example 2: (ax^2 + bx + c = 0)&lt;/p&gt;
&lt;p&gt;When we do machine learning, a lot of time, we use vectors to perform any computation. The same thing can be done using the traditional method of loops too. Let&amp;rsquo;s see how.&lt;/p&gt;
&lt;p&gt;We will write a small code to take the dot problem of two numbers. For the vectorized method, we&amp;rsquo;ll use numpy library and for non-vectorized method, we&amp;rsquo;ll use the traditional for loop.&lt;/p&gt;
&lt;h1 id=&#34;vectorized-method&#34;&gt;Vectorized Method:&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;import numpy as np
import time

#We&#39;ll create an array of the size 100000 and populate it with random samples from a uniform distribution over [0, 1).
#For this purpose, we&#39;l use np.random.rand()
#Link: https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.random.rand.html
a =  np.random.rand(100000)
b =  np.random.rand(100000)

#We&#39;ll compute the time taken to compute the vector dot product
tic = time.time()
c = np.dot(a,b)
toc = time.time()

print(c)
print(&amp;quot;Vectorized version:&amp;quot; + str(1000*(toc-tic)) + &amp;quot; ms&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Output:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;25032.80826579146
Vectorized version:5.956172943115234 ms
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;non-vectorized-method&#34;&gt;Non-Vectorized Method:&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;## Non-Vectorized Version

c = 0; tic = 0; toc = 0

#We&#39;ll compute the time taken to compute the dot product using for loop
tic = time.time()
for i in range(100000):
    c += a[i]*b[i]
toc = time.time()

print(c)
print(&amp;quot;Vectorized version:&amp;quot; + str(1000*(toc-tic)) + &amp;quot; ms&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Output:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;25032.808265791115
Vectorized version:91.36772155761719 ms
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the outputs, we can see that the for loop takes 20x time than the vectorized method. This difference can be significant if we have large amounts of data where vectorization can save us a lot of computation time. Hence, the need of vectorization!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why do we use vectorization in Machine Learning?</title>
      <link>https://srishti.dev/post/test/</link>
      <pubDate>Sun, 10 Nov 2019 12:38:00 -0700</pubDate>
      <guid>https://srishti.dev/post/test/</guid>
      <description>&lt;p&gt;When we do machine learning, a lot of time, we use vectors to perform any computation. The same thing can be done using the traditional method of loops too. Let&amp;rsquo;s see how.&lt;/p&gt;
&lt;p&gt;We will write a small code to take the dot problem of two numbers. For the vectorized method, we&amp;rsquo;ll use numpy library and for non-vectorized method, we&amp;rsquo;ll use the traditional for loop.&lt;/p&gt;
&lt;h1 id=&#34;vectorized-method&#34;&gt;Vectorized Method:&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;import numpy as np
import time

#We&#39;ll create an array of the size 100000 and populate it with random samples from a uniform distribution over [0, 1).
#For this purpose, we&#39;l use np.random.rand()
#Link: https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.random.rand.html
a =  np.random.rand(100000)
b =  np.random.rand(100000)

#We&#39;ll compute the time taken to compute the vector dot product
tic = time.time()
c = np.dot(a,b)
toc = time.time()

print(c)
print(&amp;quot;Vectorized version:&amp;quot; + str(1000*(toc-tic)) + &amp;quot; ms&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Output:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;25032.80826579146
Vectorized version:5.956172943115234 ms
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;non-vectorized-method&#34;&gt;Non-Vectorized Method:&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;## Non-Vectorized Version

c = 0; tic = 0; toc = 0

#We&#39;ll compute the time taken to compute the dot product using for loop
tic = time.time()
for i in range(100000):
    c += a[i]*b[i]
toc = time.time()

print(c)
print(&amp;quot;Vectorized version:&amp;quot; + str(1000*(toc-tic)) + &amp;quot; ms&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Output:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;25032.808265791115
Vectorized version:91.36772155761719 ms
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the outputs, we can see that the for loop takes 20x time than the vectorized method. This difference can be significant if we have large amounts of data where vectorization can save us a lot of computation time. Hence, the need of vectorization!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Feature Vector in Machine Learning</title>
      <link>https://srishti.dev/post/2019-11-07-feature-vector-in-machine-learning/</link>
      <pubDate>Thu, 07 Nov 2019 22:21:06 -0700</pubDate>
      <guid>https://srishti.dev/post/2019-11-07-feature-vector-in-machine-learning/</guid>
      <description>&lt;p&gt;In computer vision we keep referring to feature vector, especially when we talk about CNNs. But what exactly is a feature vector and can how can you visualize it?
Imagine we have an RGB image is the form as shown below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://srishti.dev/img/rgbchannel.png &#34; width=&#34;150&#34; height=&#34;120&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice, that the value are unique for each color channel. However, we don’t intend to send out this data to any algorithm in 3 fold i.e. sending for one channel and doing computation, sending for second and so on. This is very unrealistic from computation point of you because our original data (pixel values) are ‘together’ representing the image. Hence, we create what we call a feature vector. We create 1-D column vector with values from these 3 color channels. For instance, we start from Red channel and bread each value row wise till the last value. Next we do the same for Blue channel and lastly for Green channel. This gives us one single column vector representing the original image in the form of, what we call, ‘feature vector’. So, what is the final size of the feature vector? This is important to know the total amount of data we will eventually use to do any computation.&lt;/p&gt;
&lt;p&gt;When we see the CNN structure, we see that the architecture is defined usingm x n x 3, in case of RGB images. For example, a CNN architecture of dimension 64 x 64 x 3 will have a feature vector of size 12,288.&lt;/p&gt;
&lt;p&gt;Simple!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Setting up LaTeX in Jekyll</title>
      <link>https://srishti.dev/post/2019-11-08-latex-in-jekyll/</link>
      <pubDate>Thu, 07 Nov 2019 00:15:06 -0700</pubDate>
      <guid>https://srishti.dev/post/2019-11-08-latex-in-jekyll/</guid>
      <description>&lt;p&gt;Coming soon&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Decoding Tensorflow</title>
      <link>https://srishti.dev/post/2019-10-20-decode-tensorflow-language/</link>
      <pubDate>Wed, 06 Nov 2019 01:25:06 -0700</pubDate>
      <guid>https://srishti.dev/post/2019-10-20-decode-tensorflow-language/</guid>
      <description>&lt;p&gt;Tensorflow is now being used significantly for machine learning modelling, training and testing purposes. However, for someone who may have never used TensorFlow before or for people who love to understand the details behind the meanings of the commands and keywords, it may be toime consuming. This blog post is intended for people who may want to dive deep into what the commands stand for in the first place.&lt;/p&gt;
&lt;p&gt;For covenience, we will take the standard example of a CNN demonstrated on the official website 
&lt;a href=&#34;%22https://www.tensorflow.org/tutorials/images/cnn%22&#34;&gt;here&lt;/a&gt; . We will go through them step by step and decode what lies behind these lines. You can also read this blog while trying to using the colab 
&lt;a href=&#34;https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/cnn.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;importing-tensorflow&#34;&gt;Importing TensorFlow&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;from __future__ import absolute_import, division, print_function, unicode_literals
import tensorflow as tf
from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;downloading-and-working-with-cifar-dataset&#34;&gt;Downloading and working with CIFAR Dataset&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;normalize-pixel-values-to-be-between-0-and-1&#34;&gt;Normalize pixel values to be between 0 and 1&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;train_images, test_images = train_images / 255.0, test_images / 255.0
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;verification-of-the-data&#34;&gt;Verification of the Data&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;class_names = [&#39;airplane&#39;, &#39;automobile&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;,
               &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;]

plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_images[i], cmap=plt.cm.binary)
    # The CIFAR labels happen to be arrays, 
    # which is why you need the extra index
    plt.xlabel(class_names[train_labels[i][0]])
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;cnn-architecture&#34;&gt;CNN Architecture&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation=&#39;relu&#39;, input_shape=(32, 32, 3)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;))
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Sigmoid Function in Deep Learning : Why?</title>
      <link>https://srishti.dev/post/2019-11-07-sigmoid-in-deep-learning/</link>
      <pubDate>Wed, 06 Nov 2019 01:25:06 -0700</pubDate>
      <guid>https://srishti.dev/post/2019-11-07-sigmoid-in-deep-learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Joined WiCV 2020 as Chair</title>
      <link>https://srishti.dev/news/2019-11-08-latex-in-jekyll/</link>
      <pubDate>Mon, 09 Sep 2019 00:15:06 -0700</pubDate>
      <guid>https://srishti.dev/news/2019-11-08-latex-in-jekyll/</guid>
      <description>&lt;p&gt;I have joined the the organizing committee of 
&lt;a href=&#34;https://sites.google.com/view/wicvworkshop-cvpr2020/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Women in Computer Vision Workshop, 2020&lt;/a&gt; as one of the chairs. As a part of the program committte we are responsible for publicity and webmaster (maintaining social media, website, creating print materials), sponsorship, mentoring dinner (organising the dinner and mentor-mentee matching) , working with CMT (handling paper submission, reviewing and notification), finance and budgeting and bringing speakers for the workshop.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What not to do</title>
      <link>https://srishti.dev/post/2019-04-24-what-not-to-do/</link>
      <pubDate>Wed, 24 Apr 2019 16:25:06 -0700</pubDate>
      <guid>https://srishti.dev/post/2019-04-24-what-not-to-do/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed quam metus, commodo sit amet ante a, finibus efficitur lorem. Maecenas egestas purus in tempor volutpat. Sed dapibus tortor nec sem suscipit ullamcorper. Nulla nec lorem lacus. Phasellus condimentum massa quis dolor consequat viverra ut ac magna. Ut a consequat nisi. Vivamus at leo ut turpis convallis lacinia. Curabitur eu placerat quam. Donec ultricies faucibus dui, a tincidunt lorem lobortis condimentum.&lt;/p&gt;
&lt;p&gt;Quisque aliquet consectetur justo sit amet convallis. Nunc vel aliquet ipsum, sit amet elementum justo. Vivamus id magna mi. Cras luctus est vel ipsum sagittis pellentesque. Vivamus ante elit, porttitor vitae quam quis, fermentum malesuada risus. Integer nec lectus vel lacus cursus tristique in euismod ipsum. Duis ut varius enim. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Morbi et elit eu tortor lacinia sollicitudin non in lacus.&lt;/p&gt;
&lt;p&gt;Integer at viverra turpis. Duis aliquam mattis sapien tempor eleifend. Mauris nec eleifend risus, quis mollis neque. Vivamus non dapibus justo, vel ornare magna. Nunc in nulla venenatis, imperdiet diam in, accumsan massa. Etiam congue augue ipsum, sit amet rutrum nisi blandit quis. Cras in lectus non lorem auctor consequat a vel sem. Aliquam erat volutpat. Fusce lobortis vel orci vitae ullamcorper. Phasellus id eleifend eros. Mauris vulputate, nisi vel auctor auctor, quam enim tincidunt felis, vitae fermentum odio tortor eget est. Integer ornare blandit lectus a accumsan.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
