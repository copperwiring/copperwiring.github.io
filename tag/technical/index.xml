<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>technical | Srishti Yadav</title>
    <link>https://srishti.dev/tag/technical/</link>
      <atom:link href="https://srishti.dev/tag/technical/index.xml" rel="self" type="application/rss+xml" />
    <description>technical</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 11 Nov 1111 12:38:00 -0700</lastBuildDate>
    <image>
      <url>https://srishti.dev/images/icon_hu83f24c6794b0c645cb0fb9e8b29ab31d_111941_512x512_fill_lanczos_center_2.png</url>
      <title>technical</title>
      <link>https://srishti.dev/tag/technical/</link>
    </image>
    
    <item>
      <title>CLIP: Connecting Text and Images</title>
      <link>https://srishti.dev/post/1111-11-11-clip-connecting-text-and-images/</link>
      <pubDate>Sat, 11 Nov 1111 12:38:00 -0700</pubDate>
      <guid>https://srishti.dev/post/1111-11-11-clip-connecting-text-and-images/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://srishti.dev/img/clip-openai.png&#34; width=&#34;920&#34; height=&#34;720&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: Learning Transferable Visual Models From Natural Language Supervision&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PDF: 
&lt;a href=&#34;https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning Transferable Visual Models From Natural Language Supervision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Blog: 
&lt;a href=&#34;https://openai.com/blog/clip/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CLIP: Connecting Text and Images&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The paper describes an approach where it takes a large dataset of image text pair and tries to learn a model that scores whether a image and text could co-occur. This is learned over a large dataset.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Question: How to do classification zero shot i.e. without any training?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Given a specified classification task where there are some images and some labels and you are supposed to evaluate based on our prediction, you will embed all text labels into vectors and images into vectors and them compare the score of their cross product. The pairing that is going to give the highest score is going to be the prediction label.&lt;/p&gt;
 &lt;figure&gt;
    &lt;img src=&#34;https://srishti.dev/img/clip-architecture.png&#34; width=&#34;920&#34; height=&#34;720&#34;/&gt;
    &lt;figcaption&gt;
    &lt;a href=&#34;https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf&#34;&gt; Fig 1. Photo via Open AI Paper&lt;/a&gt;
    &lt;/figcaption&gt; 
 &lt;/figure&gt;
&lt;p&gt;In Fig.1, $N$ is the size of images associated with some text. $T_i$ is the encoding for the entire text string.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why is it zero shot?&lt;/strong&gt;
This model is zero shot because it refers to the number of lables we see for training. In this architecture, for a particular evaluation task, you do not train. You take the train weights(and give some bias). THis notion of zero shot is in line with GPT notion of zero shot i.e. you do a lot of training -  you don&amp;rsquo;t care what is happenng at the pre-training stage&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dataset:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Called WiT Dataset (WebImageText)&lt;/li&gt;
&lt;li&gt;400 million (image, text) pairs by searching over text queries&lt;/li&gt;
&lt;li&gt;500,000 text queries:
&lt;ul&gt;
&lt;li&gt;All words occuring atleast 100 times in the English version of Wikipedia + WordNet synsets + some details&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cap at a maximum of 20000 (image, text) pairs per query&lt;/li&gt;
&lt;li&gt;Total wordcount similar to WebText dataset used by GPT-2&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;u&gt;Note&lt;/u&gt;: There is a difference between queries and labels.
Example, when you give a query &amp;lsquo;dog&amp;rsquo; on google image, it will come with all sorts of images of &amp;lsquo;dog&amp;rsquo; and each such image will come associated with a (paired)text. These texts can be in the form of alt text or title of the page.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Model architecture:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;(In progress)
&lt;br/&gt;
&lt;u&gt;Acknowledment:&lt;/u&gt; Thanks to the discussion in TTIC reading group which introduced me to this paper.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Datasets for Fine-Grained Image Classification</title>
      <link>https://srishti.dev/post/1111-11-11-datasets-for-fine-grained-image-classification/</link>
      <pubDate>Sat, 11 Nov 1111 12:38:00 -0700</pubDate>
      <guid>https://srishti.dev/post/1111-11-11-datasets-for-fine-grained-image-classification/</guid>
      <description>&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;iNat2017&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;u&gt;Data:&lt;/u&gt; &lt;a href=&#34;https://github.com/visipedia/inat_comp/tree/master/2017&#34;&gt;&lt;a href=&#34;https://github.com/visipedia/inat_comp/tree/master/2017&#34;&gt;https://github.com/visipedia/inat_comp/tree/master/2017&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;iNat2018 and iNat2019&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;u&gt;Data:&lt;/u&gt; &lt;a href=&#34;https://github.com/visipedia/inat_comp/blob/master/2018/README.md&#34;&gt;&lt;a href=&#34;https://github.com/visipedia/inat_comp/blob/master/2018/README.md&#34;&gt;https://github.com/visipedia/inat_comp/blob/master/2018/README.md&lt;/a&gt; &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Data:&lt;/u&gt; &lt;a href=&#34;https://github.com/visipedia/inat_comp&#34;&gt;&lt;a href=&#34;https://github.com/visipedia/inat_comp&#34;&gt;https://github.com/visipedia/inat_comp&lt;/a&gt; &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Details:&lt;/u&gt; The dataset is similar to iNat2017 with small differences, which are mentioned in the website.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;Herbarium Dataset&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;u&gt;Paper:&lt;/u&gt; &lt;a href=&#34;https://drive.google.com/file/d/1HPyY82IwGkKlp3ow13JCDtn1G-s_mgGJ/view&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1HPyY82IwGkKlp3ow13JCDtn1G-s_mgGJ/view&#34;&gt;https://drive.google.com/file/d/1HPyY82IwGkKlp3ow13JCDtn1G-s_mgGJ/view&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;&lt;strong&gt;Cassava (leaves) images&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;u&gt;Paper:&lt;/u&gt; &lt;a href=&#34;https://drive.google.com/file/d/1GW0Ak_fS0ZMXcy89B7di1xNF1MBIga_4/view&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1GW0Ak_fS0ZMXcy89B7di1xNF1MBIga_4/view&#34;&gt;https://drive.google.com/file/d/1GW0Ak_fS0ZMXcy89B7di1xNF1MBIga_4/view&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;&lt;strong&gt;Birds&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;u&gt;Paper:&lt;/u&gt; &lt;a href=&#34;https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Horn_Building_a_Bird_2015_CVPR_paper.pdf&#34;&gt;&lt;a href=&#34;https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Horn_Building_a_Bird_2015_CVPR_paper.pdf&#34;&gt;https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Horn_Building_a_Bird_2015_CVPR_paper.pdf&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;&lt;strong&gt;Animal Species (camera trap)&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;u&gt;Paper:&lt;/u&gt; &lt;a href=&#34;https://arxiv.org/pdf/2004.10340.pdf&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2004.10340.pdf&#34;&gt;https://arxiv.org/pdf/2004.10340.pdf&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;7&#34;&gt;
&lt;li&gt;&lt;strong&gt;UCSD Birds 200&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;u&gt;Data:&lt;/u&gt; &lt;a href=&#34;http://www.vision.caltech.edu/visipedia/CUB-200.html&#34;&gt;&lt;a href=&#34;http://www.vision.caltech.edu/visipedia/CUB-200.html&#34;&gt;http://www.vision.caltech.edu/visipedia/CUB-200.html&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Paper:&lt;/u&gt; &lt;a href=&#34;https://authors.library.caltech.edu/27452/1/CUB_200_2011.pdf&#34;&gt;&lt;a href=&#34;https://authors.library.caltech.edu/27452/1/CUB_200_2011.pdf&#34;&gt;https://authors.library.caltech.edu/27452/1/CUB_200_2011.pdf&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;8&#34;&gt;
&lt;li&gt;&lt;strong&gt;Birdsnap Large-scale Fine-grained Visual Categorization of Birds&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;u&gt;Data:&lt;/u&gt; &lt;a href=&#34;http://birdsnap.com/&#34;&gt;Link doesn&amp;rsquo;t work anymore.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Paper:&lt;/u&gt; &lt;a href=&#34;&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2014/papers/Berg_Birdsnap_Large-scale_Fine-grained_2014_CVPR_paper.pdf&#34;&gt;https://openaccess.thecvf.com/content_cvpr_2014/papers/Berg_Birdsnap_Large-scale_Fine-grained_2014_CVPR_paper.pdf&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;9&#34;&gt;
&lt;li&gt;&lt;strong&gt;Stanford Dogs&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;u&gt;Data:&lt;/u&gt; &lt;a href=&#34;http://vision.stanford.edu/aditya86/ImageNetDogs/&#34;&gt;&lt;a href=&#34;http://vision.stanford.edu/aditya86/ImageNetDogs/&#34;&gt;http://vision.stanford.edu/aditya86/ImageNetDogs/&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Paper:&lt;/u&gt; &lt;a href=&#34;https://people.csail.mit.edu/khosla/papers/fgvc2011.pdf&#34;&gt;&lt;a href=&#34;https://people.csail.mit.edu/khosla/papers/fgvc2011.pdf&#34;&gt;https://people.csail.mit.edu/khosla/papers/fgvc2011.pdf&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;10&#34;&gt;
&lt;li&gt;&lt;strong&gt;Oxford Dogs&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;u&gt;Data:&lt;/u&gt;&lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/data/pets/&#34;&gt;&lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/data/pets/&#34;&gt;https://www.robots.ox.ac.uk/~vgg/data/pets/&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Paper:&lt;/u&gt;&lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf&#34;&gt;&lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf&#34;&gt;https://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;11&#34;&gt;
&lt;li&gt;&lt;strong&gt;Flowers&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;u&gt;Data:&lt;/u&gt;&lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/data.html&#34;&gt;Link mentioned in the paper but doesn&amp;rsquo;t work. Couldn&amp;rsquo;t find the datasest&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Paper:&lt;/u&gt;&lt;a href=&#34;https://www.ics.uci.edu/~welling/teaching/273ASpring09/nilsback06.pdf&#34;&gt;&lt;a href=&#34;https://www.ics.uci.edu/~welling/teaching/273ASpring09/nilsback06.pdf&#34;&gt;https://www.ics.uci.edu/~welling/teaching/273ASpring09/nilsback06.pdf&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Few Shot Learning</title>
      <link>https://srishti.dev/post/1111-11-11-few-shot-learning-basic-concepts/</link>
      <pubDate>Sat, 11 Nov 1111 12:38:00 -0700</pubDate>
      <guid>https://srishti.dev/post/1111-11-11-few-shot-learning-basic-concepts/</guid>
      <description>&lt;p&gt;&lt;b&gt;Question&lt;/b&gt;: If a class has only two samples, can a computer make correct prediction? &lt;br&gt;
&lt;i&gt;Note: Number of samples is too less for training.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Approach:&lt;/b&gt; Few Shot Learning&lt;/p&gt;
&lt;p&gt;Few shot learning is a problem where we try to learn when the training data is very small. It is different from supervised learning where we train on some data and try to predict an object which belongs to a class present in training data. In few shot learning, the training data has never the sample we are predicting on.&lt;/p&gt;
&lt;p&gt;Le&amp;rsquo;s take an example. Look at Fig.1&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://srishti.dev/img/half-training-data.jpg&#34; width=&#34;300&#34; height=&#34;400&#34; /&gt;
    &lt;figcaption&gt;
    &lt;a href=&#34;https://cs231n.github.io/classification/&#34;&gt; Fig 1. Photo via CS231 course&lt;/a&gt;
    &lt;/figcaption&gt; 
 &lt;/figure&gt;
&lt;p&gt;We train a model of a big training set of cats, dogs, mug and hat. The goal in few shot learning is not to recognize unseen cats/dogs/mugs/hats. Instead the goal is to recognize the similarity and difference between objects. After training, if we show two pairs of images (Fig.2 and Fig.3) to the model and ask &amp;ldquo;&lt;em&gt;Are the two images similar?&lt;/em&gt;&amp;quot;.&lt;/p&gt;
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;style&gt;
* {
  box-sizing: border-box;
}
.column img {
    width: 100px;
    height: 150px;
    float: left ;
    padding: 10px; 
}
/* Clearfix (clear floats) */
.row::after {
  content: &#34;&#34;;
  clear: both;
  display: table;
}
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;div class=&#34;row&#34;&gt;
    &lt;div class=&#34;column&#34;&gt;
        &lt;img src=&#34;https://srishti.dev/img/cat.png&#34; width=&#34;300&#34; height=&#34;600&#34; style=&#34;width:120%&#34;&gt;
        &lt;figcaption&gt;
            &lt;a href=&#34;https://medium.com/fenwicks/tutorial-4-demystifying-keras-dogs-vs-cats-tutorial-f7c5ea7adcf8&#34;&gt; Fig 2. Photo via Medium blog&lt;/a&gt;
        &lt;/figcaption&gt; 
    &lt;/div&gt;
    &lt;div class=&#34;column&#34;&gt;
        &lt;img src=&#34;https://srishti.dev/img/cat-dog.jpeg&#34; width=&#34;300&#34; height=&#34;600&#34; style=&#34;width:100%&#34; &gt;
        &lt;figcaption&gt;
            &lt;a href=&#34;https://cdn-images-1.medium.com/max/1600/1*EvMbMNRHm_aOf1n4tDO1Xg.jpeg&#34;&gt; Fig 3. Photo via Medium blog&lt;/a&gt;
        &lt;/figcaption&gt;  
    &lt;/div&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;Since the model has learned the similarity and difference between objects, it can tell that images in Fig.2 are same kind of objects and Fig.3 are quite different. Now, if you ask the model to &lt;em&gt;recognize&lt;/em&gt; the objects, it does not know it is cat or dog because it isn&amp;rsquo;t a part of the training data.  Hence, model can tell that these two images are similar but can&amp;rsquo;t tell what they are.&lt;/p&gt;
&lt;p&gt;Now, let&amp;rsquo;s ask another question. Let&amp;rsquo;s ask the model what is Fig. 4. We call this image: &lt;strong&gt;Query&lt;/strong&gt;&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://srishti.dev/img/squirrel.jpg&#34; width=&#34;300&#34; height=&#34;400&#34; /&gt;
    &lt;figcaption&gt;
    &lt;a href=&#34;http://weknowyourdreams.com/single/squirrel/squirrel-09&#34;&gt; Fig 4. &#34;Query&#34; (Photo via web)&lt;/a&gt;
    &lt;/figcaption&gt; 
 &lt;/figure&gt;
&lt;p&gt;Model is unable to answer this question because it has never seen this data during training. Now, I show another 4 images to the model as shown in Fig. 5&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://srishti.dev/img/support-set.png&#34; width=&#34;600&#34; height=&#34;800&#34; /&gt;
    &lt;figcaption&gt;
    &lt;a&gt; Fig 5. &#34;Support-Set&#34;&lt;/a&gt;
    &lt;/figcaption&gt; 
 &lt;/figure&gt;
&lt;p&gt;Now, model compares the query image with each image in the support set and believes the query is &amp;ldquo;Squirre&amp;rdquo;. These set of labelled images are called &lt;strong&gt;&amp;ldquo;Support Set&amp;rdquo;&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Jargons related to few-shot learning&lt;/p&gt;
&lt;!-- Let&#39;s define few terms before we get into more technical details: --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;k-way:&lt;/b&gt; we are given $j$ classes (e.g. 3-way means 3 classes. See Fig.1)&lt;/li&gt;
&lt;li&gt;&lt;b&gt;n-shot:&lt;/b&gt; the number of samples per class (e.g. 2-shot mean 2 sample per class. See Fig.3)&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Support set($S$):&lt;/b&gt; the samples used for learning. If we have $j$ classes and $k$ samples, then number of elements in support set is $ j * k $.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Query set ($Q$):&lt;/b&gt; the sample(s) which we are trying to identify.&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://srishti.dev/img/support_query.png &#34; width=&#34;600&#34; height=&#34;800&#34; /&gt;
    &lt;figcaption&gt;
    &lt;a href=&#34;https://www.borealisai.com/en/blog/tutorial-2-few-shot-learning-and-meta-learning-i/&#34;&gt; Fig 3. Photo via Borealis AI blog&lt;/a&gt;
    &lt;/figcaption&gt; 
 &lt;/figure&gt;
&lt;p&gt;More formally,if we define task with $T$, then task $T_i$ is sampled from probabilty distribution over tasks&lt;/p&gt;
&lt;div&gt; $$ T_i\sim P(T) $$&lt;/div&gt;  and each task $i$ is a set given by Support $S$ and Query $Q$
&lt;div&gt;$$ T_i = \{ S_i, Q_i \} $$&lt;/div&gt;
&lt;!-- Points:
- If we already already pre-trained network on say,imagnet, it appears that if we simply use &#39;traditional&#39; transfer learning, we shold be able to achieve  better results in j-way k-shot learning. However, it doesn&#39;t necessarily happen. &lt;br /&gt; &lt;br /&gt;
We are dealing with domain transfer. First, there is the problem of domain shift. You may be in a new setting where your model cannot generalize to the new classes (classes different from the one in ImageNet for instance). Few-Shot models better deal with this setting. Second issue, in transfer learning you have to retrain part of the model (e.g. the last layer) and to do so you have to find the right number of epochs, optimizer, and hyperparams. This is very complicated to do when you have just a handful of data like in the Few-Shot setting. --&gt;
</description>
    </item>
    
    <item>
      <title>Hyperopt: A tool for parameter tuning</title>
      <link>https://srishti.dev/post/1111-11-11-hyperopt/</link>
      <pubDate>Sat, 11 Nov 1111 12:38:00 -0700</pubDate>
      <guid>https://srishti.dev/post/1111-11-11-hyperopt/</guid>
      <description>&lt;p&gt;In deep learning, it is not easy to tune hyperparameters for optimal results. If we have 2 parameters (each with 3 prior desirable values), it is an easier problem. We will have possible combinations to try. However, with more parameters, the possible combinations will increase exponentially. For example, for 5 parameters, each with 4 desired values, we will have possible combinations. Manually trying each of them is not a very practical approach.&lt;/p&gt;
&lt;p&gt;hence, the question usually is &amp;ldquo;Which way should I update my hyper-parameter to reduce the loss (i.e. gradients) in order to find the optimal model architecture?&lt;/p&gt;
&lt;p&gt;Idealy, we should come up with an approach :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;where we can search for hyperparameters (hyper-parameter search space) using a distributed compute on the cloud.&lt;/li&gt;
&lt;li&gt;intelligently optimize which of the possible combinations from the search space will give us the best results.&lt;/li&gt;
&lt;li&gt;Supports exisitng optimization techniques like Grid Search, Random Search, Bayesian Optmization etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some of the exisiting tools for hyperparamter tuning are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;RayTune (&lt;a href=&#34;https://ray.readthedocs.io/en/latest/tune.html&#34;&gt;https://ray.readthedocs.io/en/latest/tune.html&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Talos (&lt;a href=&#34;https://autonomio.github.io/talos&#34;&gt;https://autonomio.github.io/talos&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;NNI (&lt;a href=&#34;https://nni.readthedocs.io/en/latest/index.html&#34;&gt;https://nni.readthedocs.io/en/latest/index.html&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Orion (&lt;a href=&#34;https://orion.readthedocs.io/en/latest/user/pytorch.html#adapting-the-code-for-orion&#34;&gt;https://orion.readthedocs.io/en/latest/user/pytorch.html#adapting-the-code-for-orion&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Sherpa (&lt;a href=&#34;https://parameter-sherpa.readthedocs.io/en/latest/parallel/parallel-guide.html&#34;&gt;https://parameter-sherpa.readthedocs.io/en/latest/parallel/parallel-guide.html&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;TPOT (&lt;a href=&#34;https://github.com/EpistasisLab/tpot&#34;&gt;https://github.com/EpistasisLab/tpot&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Hyperopt (&lt;a href=&#34;https://github.com/hyperopt/hyperopt&#34;&gt;https://github.com/hyperopt/hyperopt&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;AWS Sagemaker Hyperparameter tuning (&lt;a href=&#34;https://aws.amazon.com/sagemaker/?hp=tile&amp;amp;so-exp=below&#34;&gt;https://aws.amazon.com/sagemaker/?hp=tile&amp;amp;so-exp=below&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Botorch (&lt;a href=&#34;https://botorch.org/&#34;&gt;https://botorch.org/&lt;/a&gt;)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here, we will discuss hyperopt!&lt;/p&gt;
&lt;p&gt;Hyperopt is an open-source hyperparameter tuning library written for Python. Hyperopt provides a general API for searching over hyperparameters and model types. Hyperopt offers two tuning algorithms: Random Search and the Bayesian method Tree of Parzen Estimators (TPE). To run hyperopt you define:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the objective function&lt;/li&gt;
&lt;li&gt;the parameter space&lt;/li&gt;
&lt;li&gt;the number of experiments&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are both continuous and categorical methods to describe the parameters.&lt;/p&gt;
&lt;!-- &lt;img src=&#34;https://srishti.dev/img/hyperopt.png &#34; width=&#34;600&#34; height=&#34;800&#34; /&gt; --&gt;
&lt;p&gt;&lt;strong&gt;Limitation&lt;/strong&gt;:
Hyper-parameter takes a lot of time to tune the parameters if number of trials and number of epocs (iteration of the neural network) are higher (which is desired). Hence, it would be good to explore how to parallelize the tuning work.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>KL-Divergence</title>
      <link>https://srishti.dev/post/1111-11-11-kl-divergence/</link>
      <pubDate>Sat, 11 Nov 1111 12:38:00 -0700</pubDate>
      <guid>https://srishti.dev/post/1111-11-11-kl-divergence/</guid>
      <description>&lt;p&gt;&lt;strong&gt;What is KL-Divergence&lt;/strong&gt;
KL Divergence is a measure of how one probabilty distributon is different from another. Some people also call it the distance between two distributions, however, strictly speaking it is not the distance. Distance is commutative while KL-Divergence is not.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mathematically&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$ D_{KL}(P||Q) = \sum_x P(X= x) log\frac{P(X=x)}{Q(X=x)} $$&lt;/p&gt;
&lt;p&gt;$$ D_{KL}(P||Q) = \sum_x P(X= x) log\frac{P(X=x)}{Q(X=x)} $$&lt;/p&gt;
&lt;p&gt;where $\sum_x P(X= x)$ is the summation of all the values that random variable $X$ will take and $P(X= x)$ is the probabilty of that random variable. In short we can also write:&lt;/p&gt;
&lt;p&gt;$$ D_{KL}(P||Q) = \sum_x P(x) log\frac{P(x)}{Q(x)} $$&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;How can we generalize it to two different distributions?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Suppose we have two multivariate normal distributions defined as&lt;/p&gt;
&lt;p&gt;$$ p(x) = N(x;p_1, \Sigma_1 ) $$
$$ q(x) = N(x;p_2, \Sigma_2 ) $$&lt;/p&gt;
&lt;p&gt;where $N$ is the normal distribution, $p_1$ &amp;amp; $p_2$ are are the means and $\Sigma_1$ and $\Sigma_2$ are the covariance matrix.&lt;/p&gt;
&lt;p&gt;The multivariate normal distribution is defined as:&lt;/p&gt;
&lt;p&gt;$$ N(x;p;\Sigma_1) = \frac{1}{\sqrt{(2\pi)^{k} |\Sigma_1|}} * exp(- \frac{1}{2} (x-p)^{T} \Sigma^{-1}(x-p)) $$&lt;/p&gt;
&lt;p&gt;if the two distributions have the same distributions. Here $x$ is the vector of length $k$ x $1$ i.e. the elements are $x= [x_1, x_2&amp;hellip;x_n]^{T}$&lt;/p&gt;
&lt;p&gt;Now, if we have two distributions as mutivariate normal density, then KL Divergence between the two normal distributions is defined as:&lt;/p&gt;
&lt;p&gt;$$ D_{KL}(p(x) || q(x)) = \frac{1}{2} {  \bigg[log \frac{|\Sigma_{2}|}{|\Sigma_{1}|} - d + tr(\Sigma_2^{-1}\Sigma_1) + (p_2 - p_1)^{T} \Sigma_2^{-1}(p_2 - p_1)} \bigg] $$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;We know that KL Divergence between two PDFs can be expressed as:
$$ D_{KL}(p(x) || q(x)) = \sum_x P(x) log\frac{P(x)}{Q(x)} \tag{1}  $$&lt;/p&gt;
&lt;p&gt;and multi-variate normal distribution is defined like:
$$ N(x;p;\Sigma_1) = \frac{1}{\sqrt{(2\pi)^{k} |\Sigma_1|}} * exp\Big[- \frac{1}{2} (x-p_1)^{T} \Sigma^{-1}(x-p_1)\Big] \tag{2}  $$&lt;/p&gt;
&lt;!-- See \ref{1} for a how-to.  --&gt;
&lt;p&gt;&lt;em&gt;Note: $|\Sigma|$ is the determinant and is not the absolute value&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Taking log of Eq.$2$, we can write as:&lt;/p&gt;
&lt;p&gt;$$ log P(x) = - \frac{k}{2}log(2\pi) - \frac{1}{2}log(|\Sigma_1|) - \frac{1}{2} \Big[(x-p_1)^{T} \Sigma^{-1}(x-p_1)\Big]  \tag{3} $$&lt;/p&gt;
&lt;p&gt;Similarly for second probabilty distribution $Q$,
$$ log Q(x) = - \frac{k}{2}log(2\pi) - \frac{1}{2}log(|\Sigma_2|) - \frac{1}{2} \Big[(x-p_2)^{T} \Sigma^{-1}(x-p_2)\Big]  \tag{4} $$&lt;/p&gt;
&lt;p&gt;Now, Eq.$\tag{1}$ can be re-written as:
$$ D_{KL}(p(x) || q(x)) = \sum_x P(x) * [logP(x) - logQ(x)] \tag{5}  $$&lt;/p&gt;
&lt;p&gt;Substituting Eq. $3$ and Eq. $4$ in Eq. $5$, we get:&lt;/p&gt;
&lt;div&gt;
$$ 
\begin{multline} 
 D_{KL}(p(x) || q(x)) = 
\sum_x P(x) *  \\
\hspace{25mm}- \Big[\frac{k}{2}log(2\pi) - \frac{1}{2}log(|\Sigma_1|) - \frac{1}{2} \Big[(x-p_1)^{T} \Sigma^{-1}(x-p_1)\Big] \Big] \\
\hspace{-20mm} \Big[- \Big[- \frac{k}{2}log(2\pi) - \frac{1}{2}log(|\Sigma_2|) - \frac{1}{2} \Big[(x-p_2)^{T} \Sigma^{-1}(x-p_2)\Big]\Big]\Big]
\end{multline} 
$$
&lt;/div&gt;
&lt;p&gt;$$ $$&lt;/p&gt;
&lt;div&gt;
$$ 
\begin{multline} 
 D_{KL}(p(x) || q(x)) = 
\sum_x P(x) *  \\
\hspace{25mm} \xcancel{-\frac{k}{2}log(2\pi)}  \boxed{- \frac{1}{2}log(|\Sigma_1|)} - \frac{1}{2} \Big[(x-p_1)^{T} \Sigma^{-1}(x-p_1) \\
\xcancel{+\frac{k}{2}log(2\pi)} \boxed{+ \frac{1}{2}log(|\Sigma_2|)} + \frac{1}{2} \Big[(x-p_2)^{T} \Sigma^{-1}(x-p_2)\Big]
\end{multline} 
$$
&lt;/div&gt;
&lt;p&gt;We can cancel out the parts that give 0, club the boxed item in one and remaining terms in one. Hence,&lt;/p&gt;
&lt;div&gt;
$$ 
\begin{multline} 
 D_{KL}(p(x) || q(x)) = 
\sum_x P(x) *  \\
\hspace{25mm}  \Bigg[ \hspace{5mm} \boxed{+ \frac{1}{2}log(\frac{|\Sigma_2|}{|\Sigma_1|})} - \frac{1}{2} \Big[(x-p_1)^{T} \Sigma^{-1}(x-p_1) \Big] + \frac{1}{2} \Big[(x-p_2)^{T} \Sigma^{-1}(x-p_2)\Big]\Bigg]\tag{6}
\end{multline} 
$$
&lt;/div&gt;
&lt;p&gt;$P(x)$ can be split for each of the terms and hence can be written as:&lt;/p&gt;
&lt;div&gt;
$$ 
\begin{multline} 
 D_{KL}(p(x) || q(x)) = 
\sum_x P(x) * \Bigg[ \hspace{5mm} \frac{1}{2}log(\frac{|\Sigma_2|}{|\Sigma_1|})\Bigg] \\
\sum_x P(x) * \Bigg[- \frac{1}{2} \Big[(x-p_1)^{T} \Sigma^{-1}(x-p_1) \Big] \Bigg] + 
\sum_x P(x) * \Bigg[ \frac{1}{2} \Big[(x-p_2)^{T} \Sigma^{-1}(x-p_2)\Big]\Bigg] \tag{7}
\end{multline} 
$$
&lt;/div&gt;
&lt;p&gt;Now let us solve the remaining terms one by one. We will start with&lt;/p&gt;
&lt;p&gt;$$
\sum_x P(x) * \Bigg[ \frac{1}{2} \Big[(x-p_2)^{T} \Sigma_2^{-1}(x-p_2)\Big]\Bigg] \equiv E_p\Bigg[ \frac{1}{2} \Big[(x-p_2)^{T} \Sigma_2^{-1}(x-p_2)\Big]\Bigg]
$$&lt;/p&gt;
&lt;p&gt;Using properties of trace and expectation (Appendix -1), we can re-write:&lt;/p&gt;
&lt;p&gt;$$
E_p\Bigg[ \frac{1}{2} \Big[(x-p_2)^{T} \Sigma_2^{-1}(x-p_2)\Big]\Bigg]
$$ as&lt;/p&gt;
&lt;p&gt;$$
= \frac{1}{2}E_p\Bigg[ tr\Big[(x-p_2)^{T} \Sigma_2^{-1}(x-p_2)\Big]\Bigg]
= E_p\Bigg[ tr\Big[\frac{1}{2} (x-p_2)^{T} \Sigma_2^{-1}(x-p_2)\Big]\Bigg]
= E_p\Bigg[ tr\Big[\frac{1}{2}(x-p_2)(x-p_2)^{T} \Sigma_2^{-1}\Big]\Bigg]
$$&lt;/p&gt;
&lt;p&gt;Using propery __, we have&lt;/p&gt;
&lt;p&gt;$$
= tr \Bigg[\hspace{3mm}\boxed{E_p \Bigg( \Bigg[(x-p_2)(x-p_2)^{T} \Bigg]} \frac{1}{2} \Sigma_2^{-1} \Bigg) \Bigg]
$$&lt;/p&gt;
&lt;p&gt;The boxed element is nothing but a covaiance matrix $\Sigma_2$, therefore
$$
D_{KL}\Big(p(x) || q(x)\Big) =  tr \Bigg[ \Sigma_2 \frac{1}{2} \Sigma_2^{-1}] \Bigg] = tr \Bigg[ I_k \Bigg] \equiv k
$$&lt;/p&gt;
&lt;p&gt;(Equations in Latex are so tough sometimes) To be continued &amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Presence-Only Geographical Priors for Fine-Grained Image Classification</title>
      <link>https://srishti.dev/post/1111-11-11-presence-only-geographical-priors-for-fine-grained-image-classification/</link>
      <pubDate>Sat, 11 Nov 1111 12:38:00 -0700</pubDate>
      <guid>https://srishti.dev/post/1111-11-11-presence-only-geographical-priors-for-fine-grained-image-classification/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper:&lt;/strong&gt; 
&lt;a href=&#34;https://arxiv.org/abs/1906.05272&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Objective:&lt;/strong&gt; &lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explores how can we use additional meta-data available to make better classification (in this case animal species).&lt;/li&gt;
&lt;li&gt;Explores how to make best use of additional meta data which comes with most images today.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Summary:&lt;/strong&gt; &lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Knowing where a given image was taken can provide a strong prior for what objects it may contain.&lt;/li&gt;
&lt;li&gt;Paper provide a novel training loss to capture these relationship&lt;/li&gt;
&lt;li&gt;The data they assemble can have unrelated image and location dataset as long as both contain the same categories.&lt;/li&gt;
&lt;li&gt;At test time, given an image and where and when it was taken, they aim to estimate which category it contains.&lt;/li&gt;
&lt;li&gt;Location information is incorporated as bayesian spatio-temporal prior. Also, during the modelling, spatio temporal (longitude, latitude, time) are independent from the image classifier&lt;/li&gt;
&lt;li&gt;It is difficult and time consuming to have information on where and when a given category has been a.) observed to be present and b.) observed to be absent. Hence, the paper explores presence-only setting (&lt;u&gt;novelty&lt;/u&gt;)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>The iNaturalist Species Classification and Detection Dataset</title>
      <link>https://srishti.dev/post/1111-11-11-the-inaturalist-species-classification-and-detection-dataset/</link>
      <pubDate>Sat, 11 Nov 1111 12:38:00 -0700</pubDate>
      <guid>https://srishti.dev/post/1111-11-11-the-inaturalist-species-classification-and-detection-dataset/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper:&lt;/strong&gt; 
&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2018/papers/Van_Horn_The_INaturalist_Species_CVPR_2018_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Objective:&lt;/strong&gt; &lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Focuses on species of plants and animals captured in wide variety of situations, different camera types, varying image quality, feature large class imbalance and verified by citizen scientists.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Summary:&lt;/strong&gt; &lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;u&gt;Details:&lt;/u&gt; There are a total of 5,089 categories in the dataset, with 579,184 training images and 95,986 validation images. For the training set, the distribution of images per category follows the observation frequency of that category by the iNaturalist community. Therefore, there is a non-uniform distribution of images per category.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;u&gt;Experiments: &lt;/u&gt; Classification experiments were done using ResNet, Inception V3, Inception ResNet V2 and MobileNet&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;u&gt;Known issues: &lt;/u&gt; a.)  Doesn&amp;rsquo;t contains additional annotations such as sex and life stage attributes, habitat tags, and pixel level labels for the four super-classes that were challenging to annotate. b.) Need of an efficient algorithm that works when the test set contains classes that were never seen during training.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition</title>
      <link>https://srishti.dev/post/1111-11-11-the-unreasonable-effectiveness-of-noisy-data-for-fine-grained-recognition/</link>
      <pubDate>Sat, 11 Nov 1111 12:38:00 -0700</pubDate>
      <guid>https://srishti.dev/post/1111-11-11-the-unreasonable-effectiveness-of-noisy-data-for-fine-grained-recognition/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper:&lt;/strong&gt; 
&lt;a href=&#34;https://research.google/pubs/pub45605/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Objective:&lt;/strong&gt; &lt;/b&gt;
Leverage free, noisy data from the web to train effective models of fine-grained recognition.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Summary:&lt;/strong&gt; &lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Interesting paper on using noisy data from the web.&lt;/li&gt;
&lt;li&gt;They sample images directly from Google search, using all returned images as images for a given category. For L-Bird and L-Butterfly, queries are for the scientific name of the category, and for L-Aircraft and L-Dog queries are simply for the category name (e.g.“Boeing 737-200” or “Pembroke Welsh Corgi”).&lt;/li&gt;
&lt;li&gt;Active learning-based approach to collect the data.&lt;/li&gt;
&lt;li&gt;The active learning begins by training a classifier on a seed set of input images and labels (i.e.the Stanford Dogs training set), then proceeds by  iteratively  picking  a  set  of  images  to  annotate,  obtaining  labels  with  human  annotators,  and  re-training  the  classifier.&lt;/li&gt;
&lt;li&gt;Inception V3 is the base classifier&lt;/li&gt;
&lt;li&gt;To avoid images overlap between GT and web images, aggressive duplication procedure with all ground truth test sets and their corresponding web images is performed using a SOTA for learning similarity metric between images.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt; &lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How reliable are the search results for a single category from web, otherwise there are a lot of False Positives being introduced.&lt;/li&gt;
&lt;li&gt;Is it limited to extracting category information or do we need extra labels like position, time etc.&lt;/li&gt;
&lt;li&gt;How much human annotators are required to be involved or are they even involved?&lt;/li&gt;
&lt;li&gt;Since algo queries user to label data from time to time; does this query apply on a subset of the images to verify the annotations or create annotations.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>We Have So Much In Common: Modeling Semantic Relational Set Abstractions In Videos</title>
      <link>https://srishti.dev/post/1111-11-11-we-have-so-much-in-common/</link>
      <pubDate>Sat, 11 Nov 1111 12:38:00 -0700</pubDate>
      <guid>https://srishti.dev/post/1111-11-11-we-have-so-much-in-common/</guid>
      <description>&lt;p&gt;&lt;strong&gt;General Terms:&lt;/strong&gt; &lt;/b&gt;
Semantic: Semantics is the study of relationship between words and how we can draw meaning from words. Example, A child could be called a child, kid, boy, girl, son, daughter. Sematic relations connect up entities in a text. Semantic relation are at the cross-road between knowledge and language and tTogether with entities make up good chunk of the meaning of the text.&lt;/p&gt;
&lt;p&gt;To explore, how sets of group of words are linked by means of sevaeral semantic relations, one can look at 
&lt;a href=&#34;http://wordnetweb.princeton.edu/perl/webwn&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WordNet&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Gist:&lt;/strong&gt; &lt;/b&gt;
For humans it is easy to identify how two events are related by looking at them. Inherently, we can decompose two events in general abstract meaning and see which of the abstract meaning are similar. Example, when we see a.) a human being typing on a computer b.) a GIF with a mouse typing on a calculator, we can relate the hand movements and object being typed upon. We can relate that both activities are related to pushing some buttons and seeing a result on screen and hence can see the similarities in the two activities. It is a result of underlying human decision making abilty. Computers can&amp;rsquo;t do the same.&lt;/p&gt;
&lt;p&gt;The paper proposes &amp;ldquo;an approach for learning semantic relational set abstractions on videos&amp;rdquo;. &amp;ldquo;semantic relational set abstractions&amp;rdquo; should be read in two parts &amp;ldquo;semantic&amp;rdquo; and &amp;ldquo;relational set abstractions&amp;rdquo; for better understanding. It simply says that if we can find the relational sets between different videos (e.g. similar images at different view angles) and get an abstract meaning from these relational sets, the proposed method can learn the semantic (meaning) between these abstractions. If we can do so, is will be easier to tell which images are similar, which are different and how they are similar/different.&lt;/p&gt;
&lt;p&gt;More formally, as the paper puts it, this allows our model to perform cognitive tasks such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;set abstraction (which general concept is in common among a set of videos?)&lt;/li&gt;
&lt;li&gt;set completion (which new video goes well with the set?), and&lt;/li&gt;
&lt;li&gt;odd one out detection (which video does not belong to the set?)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Datsets used:&lt;/strong&gt; &lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;K400 (Kinect 400)&lt;/li&gt;
&lt;li&gt;Multi-Monets in Time&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Accessing Windows File From Linux Subshell</title>
      <link>https://srishti.dev/post/1111-11-11-accessing-windows-from-linux/</link>
      <pubDate>Sat, 11 Nov 1111 01:25:06 -0700</pubDate>
      <guid>https://srishti.dev/post/1111-11-11-accessing-windows-from-linux/</guid>
      <description>&lt;p&gt;If you are using Ubuntu subshell in Windows (I use the one available in Microsoft Store), this is for you. There are times, where you may need to access the files saved on your windows in the Linux subshell.
All you need to do is mount your drive. You may need to access the subshell via administrative privileges. For example, if you have the file in C: Drive, use the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /mnt/c
ls
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The same can be used to access data from external hard disk too. If the external disk is in D: Drive, the command would simply change to:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /mnt/d
ls
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
